---
title: "Lidl Case Study"
author: "Chris-Gabriel Islam"
date: "25/02/2021"
output: 
  html_document:
    toc: true
    toc_float: true
knit: (function(inputFile, encoding) {
  rmarkdown::render(inputFile, encoding = encoding, output_dir = "../output") 
  })
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# Package names
packages <- c("tidyr", "dplyr", "data.table", "psych", "corrplot", "car", "hdm", "MuMIn", "finalfit", "knitr", "lme4", "cAIC4", "texreg", "huxtable", "openxlsx", "lmtest")

# Install packages not yet installed
installed_packages <- packages %in% rownames(installed.packages())
if (any(installed_packages == FALSE)) {
  install.packages(packages[!installed_packages])
}

# Packages loading
invisible(lapply(packages, library, character.only = TRUE))

# tools
`%ni%` <- Negate(`%in%`)
```

## Import data
```{r import data}
data_orig <- fread('../data/candy-data_edit.csv', stringsAsFactors = FALSE)
data <- data_orig
characteristica <- "chocolate+fruity+caramel+peanutyalmondy+nougat+crispedricewafer+hard+bar+pluribus"
```

Note that we added producer manually.

## Clean and edit data

Remove one dime and one quarter as they do not belong to the original distribution and might change regression results.

```{r clean data}
data <- data[data$competitorname %ni% c("One dime", "One quarter")]
```

Because we deleted data, we have to readjust percentile rangs of pricepercent and sugarpercent. 

```{r adjust percentile ranks}
# define function
percentile.ranked <- function(a.vector, value) {
  length(sort(a.vector)[a.vector < value])/length(a.vector)
}
b <- data$sugarpercent
data$sugarpercent <- sapply(data$sugarpercent, percentile.ranked, a.vector=b) 
b <- data$pricepercent
data$pricepercent <- sapply(data$pricepercent, percentile.ranked, a.vector=b) 
```

We checked the sugar content manually and is not plausible (Reese Miniatures, Nestle Crunch, Skittles Wild Berry, Milky Way Simply Caramel). Hence, we build a robust data set without the variable sugarpercent.

```{r create robust data set}
robust <- data
robust$sugarpercent <- NULL
```

Next, we set a numerical subset for further use.

```{r numerical data}
data_num <- subset(data, select = -c(competitorname, producer))
robust_num <- subset(robust, select = -c(competitorname, producer))
```


## Descriptive Analysis

### Missing values
```{r missing values}
data %>%missing_plot()
```

No missing values.

### Frequencies

```{r frequencies}
kable(describe(data, omit=TRUE), caption = "Summary statistics", digits=3)
```
### Correlations

```{r correlations}
corrplot(as.matrix(cor(na.omit(data_num))))
# corrplot for presentation
data_num_deutsch <- subset(data_num, select=-c(sugarpercent, pricepercent))
colnames(data_num_deutsch) <- c("Schokolade", "Frucht", "Karamell", "Nuss", "Nougat", "Keksartig", "Hart", "Riegel", "Mehrteilig", "Beliebtheit")
corrplot(as.matrix(cor(na.omit(data_num_deutsch))), tl.col="black", tl.cex=1.2, tl.srt=50)
```

High negative correlation between chocolate and fruity. High positive correlation between chocolate and winpercent. Negative correlation between pluribus and bar and bar and fruity. Also positive correlation of pricepercent and winpercent

### Response functions

```{r response functions}
plot(data$winpercent, data$sugarpercent)
plot(data$winpercent, data$pricepercent)
plot(data$winpercent, data$fruity)
plot(data$winpercent, data$crispedricewafer)
plot(data$winpercent, data$chocolate)
plot(data$winpercent, data$nougat)
plot(data$winpercent, data$caramel)
plot(data$winpercent, data$pluribus)
plot(data$winpercent, data$bar)
plot(data$winpercent, data$hard)
```

### Producer

Producer frequencies

```{r plot producer}
producer_freq <- table(data$producer)
producer_freq
# save for use in presentation
write.xlsx(producer_freq, "../output/producer_freq.xlsx")
```


## Regressions

### Usual LM for characteristica

We start with an easy linear model with only characteristica.

```{r usual lm}
mod_lm <- lm(winpercent~., data_num)
summary(mod_lm)
vif(mod_lm)
resettest(mod_lm)
raintest(mod_lm)
bptest(mod_lm)
durbinWatsonTest(mod_lm)
plot(mod_lm)
```

Adjusted R-squared of `r summary(mod_lm)$adj.r.squared` is reasonable. Additionally, significant values with high effect sizes for chocolate (+), fruity (+), peanutyalmondy (+), crispedricewafer (+), hard (-) and sugarpercent (+). Negative coefficient for pricepercent seems plausible. Linearity assumptions more or less given. VIF okay. No heteroscedasticity. But raintest for linearity fails. Some variables with high leverage (Hershey Whoppers and Snickers Crisp), but could just be due to low sample size. Resettest gives a hint that model might be wrongly specified, hence add more controls.

### Check for quadratic term of pricepercent and sugarpercent

```{r quadratic pricepercent}
mod_lm_sq <- lm(winpercent~. + I(pricepercent^2) + I(sugarpercent^2), data_num)
summary(mod_lm_sq)
vif(mod_lm_sq)
resettest(mod_lm_sq)
raintest(mod_lm_sq)
bptest(mod_lm_sq)
durbinWatsonTest(mod_lm_sq)
plot(mod_lm_sq)
```

The signs of the quadratic terms might give a hint that the marginal effects are decreasing. But as their effect sizes are really small, not significant and the plots against the dependent variable (see response functions) do not give any hints, we should not overinterpret the results. Model specification test did not get better.



### Check for interaction terms

Do normal lm with interaction. Note that we do no interaction with sugarpercent and pricepercent as this would be too hard to interpret and blow the model up. Also note that we do only consider interaction of order 2 as higher orders are not available in the data set.

```{r interactions}
mod_lm_ia <- lm(paste0("winpercent~(", characteristica, ")^2+."), data_num)
summary(mod_lm_ia)
# note: cannot calculate vif as "there are aliased coefficients in the model", hence remove linear dependent
ld.vars <- attributes(alias(mod_lm_ia)$Complete)$dimnames[[1]]
mod_lm_ia2 <- lm(paste(paste0("winpercent~(", characteristica, ")^2+."), paste(ld.vars, collapse="-"),
        sep="-"), data_num)
summary(mod_lm_ia2)
resettest(mod_lm_ia2)
raintest(mod_lm_ia2)
bptest(mod_lm_ia2)
durbinWatsonTest(mod_lm_ia2)
vif(mod_lm_ia2)
```

Could be that chocolate and peanutyalmondy as well as as peanutyalmondy and bar is very good. But better do a double lasso because of colinearity problem which still exists as can be seen in the VIFs.

```{r interactions with lasso}
rs <- rlasso(paste0("winpercent~(", characteristica, ")^2+."), data_num)
selected = which(coef(rs)[-c(1:1)] !=0) # = Select relevant variables = #
formula = paste(c("winpercent ~", names(selected)), collapse = "+")
mod_lm_ia_lasso <- lm(formula, data = data_num)
summary(mod_lm_ia_lasso)
resettest(mod_lm_ia_lasso)
raintest(mod_lm_ia_lasso)
bptest(mod_lm_ia_lasso)
durbinWatsonTest(mod_lm_ia_lasso)
vif(mod_lm_ia_lasso)
plot(mod_lm_ia_lasso)
```

Chocolate and peanutyalmondy is definitely a good idea! Besides, Lasso gives a well specified model.

### Robustness with random intercepts for producer

Note that cluster sizes of one are okay, c.f. https://stats.stackexchange.com/questions/388937/minimum-sample-size-per-cluster-in-a-random-effect-model
We do not consider random slope models as the sample size is too few for this.

```{r glm}
glm_data <- subset(data, select = -c(competitorname))
# get covariates
data_tmp <- subset(glm_data, select = -c(producer, winpercent))
all_covariates <- colnames(data_tmp)
# build formula
formula = paste(c("winpercent~(1|producer)", all_covariates), collapse = "+")
# do glm
mod_glm <- lmer(formula, data = glm_data, control = lmerControl(optimizer="bobyqa"))
summary(mod_glm)
r.squaredGLMM(mod_glm)
```

Check validity of random intercepts

```{r validity random intercepts}
# ICC 
performance::icc(mod_glm) 
# cAIC
cAIC(mod_lm)
cAIC(mod_glm)
#inspect random intercepts 
ranef(mod_glm)
```

ICC above 0.10, random intercepts differ, cAIC for random intercept model is smaller, hence random intercept model valid and has better pseudo R-squared.
Results robust with random intercepts but effect sizes got smaller. Also nougat, bar and pluribus went negative, so do not produce it like this!

### Robustness without sugarpercent

```{r without sugarpercent}
mod_wo_sugar <- lm(winpercent~., robust_num)
summary(mod_wo_sugar)
resettest(mod_wo_sugar)
raintest(mod_wo_sugar)
bptest(mod_wo_sugar)
durbinWatsonTest(mod_wo_sugar)
vif(mod_wo_sugar)
plot(mod_wo_sugar)
```

The previous results for the linear model seem to be still robust. Only hard is not significant any more. But the model specification tests are better! Next look at the quadratic terms.

```{r quadratic pricepercent robust}
mod_lm_sq_wo_sugar <- lm(winpercent~. + I(pricepercent^2), robust_num)
summary(mod_lm_sq_wo_sugar)
resettest(mod_lm_sq_wo_sugar)
raintest(mod_lm_sq_wo_sugar)
bptest(mod_lm_sq_wo_sugar)
durbinWatsonTest(mod_lm_sq_wo_sugar)
vif(mod_lm_sq_wo_sugar)
plot(mod_lm_sq_wo_sugar)
```

Concerning adjusted R^2 and model specification test, this is the best model so far. Let us take a look at the random intercept model.


```{r glm wo sugar}
glm_data <- subset(robust, select = -c(competitorname))
# get covariates
data_tmp <- subset(glm_data, select = -c(producer, winpercent))
all_covariates <- colnames(data_tmp)
# build formula
formula = paste(c("winpercent~(1|producer)", all_covariates), collapse = "+")
# do glm
mod_glm_wo_sugar <- lmer(formula, data = glm_data, control = lmerControl(optimizer="bobyqa"))
summary(mod_glm_wo_sugar)
r.squaredGLMM(mod_glm_wo_sugar)
```

```{r validity random intercepts robust}
# ICC 
performance::icc(mod_glm_wo_sugar) 
# cAIC
cAIC(mod_glm_wo_sugar)
cAIC(mod_glm_wo_sugar)
#inspect random intercepts 
ranef(mod_glm_wo_sugar)
```

It is also robust. Last, check interaction terms. Do normal lm with interaction without sugarpercent:

```{r interaction robust}
mod_lm_ia_wo_sugar <- lm(paste0("winpercent~(", characteristica, ")^2+."), robust_num)
summary(mod_lm_ia_wo_sugar)
```

Same as before, but better do a double lasso because of colinearity problem.

```{r interaction lasso robust}
rs <- rlasso(winpercent~.^2, robust_num)
selected = which(coef(rs)[-c(1:1)] !=0) # = Select relevant variables = #
formula = paste(c("winpercent ~", names(selected)), collapse = "+")
mod_lm_ia_lasso_wo_sugar <- lm(formula, data = data_num)
summary(mod_lm_ia_lasso_wo_sugar)
vif(mod_lm_ia_lasso_wo_sugar)
plot(mod_lm_ia_lasso_wo_sugar)
```

Chocolate and peanutyalmondy is not important any more. But note that R-square is very low.

## Prediction

We want to find the candy with the highest winpercent. Given only the data, this is:

```{r highest winpercent}
print(data[data$winpercent == max(data$winpercent),])
```

Now we can check if there is a better solution by using the lm-model and inventing observations. We use a grid for that.

```{r grid}
inventions <- read.csv(text=paste(colnames(data_num), collapse=","))
inventions$winpercent <- NULL
for(choc in seq(0,1)){
  for(fruit in seq(0,1)){
    for(caram in seq(0,1)){
      for(peanut in seq(0,1)){
        for(noug in seq(0,1)){
          for(crisped in seq(0,1)){
            for(har in seq(0,1)){
              for(ba in seq(0,1)){
                for(plur in seq(0,1)){
                  for(sugar in seq(0,1,0.2)){
                    for(price in seq(0,1,0.2)){
                      inventions[nrow(inventions) + 1,] = c(choc, fruit, caram, peanut, noug, crisped, har, ba, plur, sugar, price)
                    }
                  }
                }
              }
            }
          }
        }
      }
    }
  }
}
```

Now we do the prediction but we use a model without sugar in order to prevent unrealistic products as this variable might be wrong. Additionally, the model specification for models without sugar were better.We do not use the interaction model as due to colinearity the prediction might be wrong. In the end we use mod_lm_sq_wo_sugar as this has the best specification tests and adjusted R^2.

```{r prediction}
inventions_w_win <- inventions
# filter with row value <= 5, i.e. max(data$sum) 
inventions_w_win$sum <- inventions_w_win$chocolate + inventions_w_win$fruity + inventions_w_win$caramel + inventions_w_win$peanutyalmondy + inventions_w_win$nougat + inventions_w_win$crispedricewafer + inventions_w_win$hard + inventions_w_win$bar + inventions_w_win$pluribus
inventions_w_win <- inventions_w_win[inventions_w_win$sum <= 5, ]
# predict
win <- predict(mod_lm_sq_wo_sugar, inventions_w_win)
inventions_w_win <- cbind(inventions_w_win, win)
inventions_w_win <- inventions_w_win[order(-win),]
head(inventions_w_win, 20)
write.xlsx(head(inventions_w_win, 20), "../output/predictions.xlsx")
```

Last, save the regression coefficients for use in the PPT

```{r regression output}
hux <- huxtablereg(list(mod_lm, mod_lm_sq, mod_lm_ia, mod_lm_ia_lasso, mod_glm, mod_wo_sugar, mod_lm_sq_wo_sugar),stars = numeric(0), single.row = TRUE)
quick_pptx(hux, file="../output/regs.pptx")
```

## Special regression

We do some special isolated regression to have a better knowledge of certain combinations, e.g. fruit gums.

### Fruit gums

```{r fruit gum}
mod_fruit_gum <- lm(winpercent~pluribus*fruity, data_num)
summary(mod_fruit_gum)
```

Both coefficients are negative, but together it seems okay. Consider that in the interaction model the fruity:pluribus coefficient was close to zero.

### Cookie based

```{r cookie based}
mod_cookie <- lm(winpercent~crispedricewafer, data_num)
summary(mod_cookie)
```

Assuming cookie-based candy is included in crispedricewafer, cookie-based candy seems to better at least as fruit gums as the coefficient is slightly higher even. This can also be seen in a model with all three variables together. But still note the low adjusted R^2 which indicates that this is not the true model.

```{r cookie based fruit}
mod_cookie <- lm(winpercent~crispedricewafer*pluribus*fruity, data_num)
summary(mod_cookie)
```
